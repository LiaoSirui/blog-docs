## 考试大纲

**集群架构，安装和配置：25%**

- 管理基于角色的访问控制（RBAC）

- 使用 Kubeadm 安装基本集群

- 管理高可用性的 Kubernetes 集群

- 设置基础架构以部署 Kubernetes 集群

- 使用 Kubeadm 在 Kubernetes 集群上执行版本升级

- 实施 etcd 备份和还原

## RBAC

题目描述：

创建一个名字为 deployment-clusterrole 且仅允许创建以下资源类型的新 ClusterRole

- Deployment
- StatefulSet
- DaemonSet

在现有的 namespace app-team1 中创建有个名为 cicd-token 的新 ServiceAccount

限于 namespace app-team1 ， 将新的 ClusterRole deployment-clusterrole 绑定到新的 ServiceAccount cicd-token

官方文档：

- <https://kubernetes.io/docs/reference/access-authn-authz/rbac/>
- <https://kubernetes.io/zh-cn/docs/reference/access-authn-authz/rbac/>

参考解答：

```bash
# 创建 ClusterRole
kubectl create clusterrole deployment-clusterrole --verb=create --resource=deployments,statefulsets,daemonsets

# 创建 ServiceAccount
kubectl create sa cicd-token -n app-team1

# 将新的 ClusterRole deployment-clusterrole 绑定到新的 ServiceAccount cicd-token
kubectl create rolebinding bind-clusterrole \
  --clusterrole=deployment-clusterrole \
  --serviceaccount=app-team1:cicd-token \
  --namespace=app-team1
```

## 节点维护

题目描述：

- 设置配置环境 kubectl config use-context ek8s
- 将名为 ek8s-node-0 (vms25)的 node 设置为不可用，并重新调度该 node 上所有运行的 pods

官方文档：

- <https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/>

- <https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/safely-drain-node/>

参考解答：

```bash
# 切换环境
kubectl config use-context ek8s

# 禁止调度
kubectl cordon ek8s-node-0

# 执行驱逐
kubectl drain ek8s-node-0 --ignore-daemonsets --delete-emptydir-data --force
```

## k8s 升级节点

题目描述：

现有的 Kubernetes 集权正在运行的版本是 1.23.6，仅将主节点上的所有 kubernetes 控制面板和组件升级到版本 1.24.8 另外，在主节点上升级 kubelet 和 kubectl；注意：不升级etcd

官方文档（搜索关键字 kubeadm upgrade）：

- <https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>
- <https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>

参考解答：

```bash
# 切换 context
kubectl config use-context mk8s

# 准备工作
# 设置为不可用
kubectl cordon k8s-master

# 驱逐节点
kubectl drain k8s-master --delete-emptydir-data --ignore-daemonsets --force

# 升级组件
# Ubuntu
apt update
apt-cache madison kubeadm
apt-mark unhold kubeadm && \
apt-get update && apt-get install -y kubeadm=kubeadm='1.24.x-*' && \
apt-mark hold kubeadm
# yum install -y kubeadm-1.24.0-0 --disableexcludes=kubernetes

# 验证下载操作正常，并且 kubeadm 版本正确&验证升级计划：
kubeadm version
kubeadm upgrade plan

# 升级
sudo kubeadm upgrade apply v1.24.0 --etcd-upgrade=false

# 升级 kubectl、kubelet
apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet='1.24.x-*' kubectl='1.24.x-*' && \
apt-mark hold kubelet kubectl
# yum install -y kubelet-1.24.0-0 kubectl-1.24.0-0 --disableexcludes=kubernetes

# 重启 kubelet
sudo systemctl daemon-reload
sudo systemctl restart kubelet

# 恢复为可调度
kubectl uncordon k8s-master

```

## etcd 备份和恢复

题目描述：

- 首 先 为 运 行 在  `https://127.0.0.1:2379` 上 的 现 有 etcd 实 例 创 建 快 照 并 将 快 照 保 存 到 `/srv/data/etcd-snapshot.db`。
- 然后还原位于 `/srv/data/etcd-snapshot-previous.db` 的现有先前快照。
- 提供了一下 TLS 证书和密钥，以通过 etcdctl 连接到服务器。 CA 证书：`/opt/KUIN00601/ca.crt` 客户端证书: `/opt/KUIN00601/etcd-client.crt` 客户端密钥: `/opt/KUIN00601/etcd-client.key`

官方文档：

- <https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/>
- <https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/>

参考解答：

```bash
# 找到 etcdctl 的位置
find / -name etcdctl

# 设置 etcdctl 的 alias 别名
alias etcdctl='/var/lib/docker/overlay2/81db967cc7079ac61b211f282300c1e4ce1e8b1eef09b2723375056cb6304501/diff/usr/local/bin/etcdctl'

# 设置 API 版本
export ETCDCTL_API=3 

# 备份
etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/opt/KUIN00601/ca.crt \
  --cert=/opt/KUIN00601/etcd-client.crt \
  --key=/opt/KUIN00601/etcd-client.key \
  snapshot save /srv/data/etcd-snapshot.db

# 验证备份快照
etcdctl --write-out=table snapshot status /cka/etcd-snapshot.db

# 恢复数据前关闭静态 pod
cd /etc/kubernets/mainfests && mv kube-* /opt/backup
etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/opt/KUIN00601/ca.crt \
  --cert=/opt/KUIN00601/etcd-client.crt \
  --key=/opt/KUIN00601/etcd-client.key \
  snapshot restore /srv/data/etcd-snapshot-previous.db \
  --data-dir=/var/lib/ectd-restore
# 恢复后更改 etcd 存储路径
vi etcd.yaml # /var/lib/ectd -> /var/lib/ectd-restore
# 恢复静态 pod
mv /opt/backup/kube-* /etc/kubernets/mainfests
# 重启 kubelet
systemctl restart kubelet
```

## 安装 k8s 集群

打开 K8s 官网中的 [Bootstrapping clusters with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/) 文档，跟随文档中的步骤进行安装即可

注意需要设置 systemd 为 docker 的 cgroup driver，参见 <https://kubernetes.io/docs/setup/production-environment/container-runtimes/>

### 初始化 master 节点

如果节点上有多个网卡，注意通过 `--apiserver-advertise-address` 参数设置 apiserver 的监听地址，该地址应为和 worker 节点同一个局域网上的地址。

如果使用了 flannel 插件，需要在 kubeadm 命令中加入 pod cidr 参数， `kubeadm init --pod-network-cidr=10.244.0.0/16`，cidr 和 <https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml> 中配置的 CIDR 一致。

### 安装 CNI 插件

采用 kubeadm 初始化集群后，需要通过 `kubectl apply -f <add-on.yaml>` 安装 CNI addon，否则加入集群的节点会一直处于 NotReady 状态。平时安装时我们会通过 k8s 在线文档导航到一个外部的 CNI 网站上，找到该 addon 的 yaml 文件。在考试时不允许访问 CNI 的网站，在下面的 K8s 文档中有安装 CNI 插件的例子，可以将网页地址加入浏览器收藏夹中。 <https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/#steps-for-the-first-control-plane-node>
